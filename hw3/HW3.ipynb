{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSE 252A Computer Vision I Fall 2019 - Homework 3\n",
    "## Instructor: Ben Ochoa\n",
    "### Assignment published on: Tuesday, October 22, 2019\n",
    "### Due on: Tuesday, November 5, 2019 11:59 pm\n",
    "\n",
    "## Instructions\n",
    "* Review the academic integrity and collaboration policies on the course website.\n",
    "  * This assignment must be completed individually.\n",
    "* All solutions must be written in this notebook.\n",
    "  * This includes the theoretical problems, for which you **must** write your answers in Markdown cells (using LaTeX when appropriate).\n",
    "  * Programming aspects of the assignment must be completed using Python in this notebook.\n",
    "* If you want to modify the skeleton code, you may do so. It has only been provided as a framework for your solution.\n",
    "* You may use Python packages (such as NumPy and SciPy) for basic linear algebra, but you may not use packages that directly solve the problem.\n",
    "  * If you are unsure about using a specific package or function, then ask the instructor and/or teaching assistants for clarification.\n",
    "* You must submit this notebook exported as a PDF. You must also submit this notebook as `.ipynb` file.\n",
    "  * Submit both files (`.pdf` and `.ipynb`) on Gradescope.\n",
    "  * **You must mark the PDF pages associated with each question in Gradescope. If you fail to do so, we may dock points.**\n",
    "* It is highly recommended that you begin working on this assignment early.\n",
    "* **Late policy: assignments submitted late will receive a 15% grade reduction for each 12 hours late (i.e., 30% per day). Assignments will not be accepted 72 hours after the due date. If you require an extension (for personal reasons only) to a due date, you must request one as far in advance as possible. Extensions requested close to or after the due date will only be granted for clear emergencies or clearly unforeseeable circumstances.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1: Photometric Stereo, Specularity Removal [20 pts]\n",
    "\n",
    "The goal of this problem is to implement a couple of different algorithms that reconstruct a surface using the concept of Lambertian photometric stereo. Additionally, you will implement the specular removal technique of [Mallick et al.](http://www.eecs.harvard.edu/~zickler/download/photodiff_cvpr05_preprint.pdf), which enables photometric stereo to be performed on certain non-Lambertian materials.\n",
    "\n",
    "You can assume a Lambertian reflectance function once specularities are removed. However, note that the albedo is unknown and non-constant in the images you will use.\n",
    "\n",
    "As input, your program should take in multiple images along with the light source direction for each image. Each image is associated with only a single light, and hence a single direction.\n",
    "\n",
    "### Data\n",
    "You will use synthetic images and specular sphere images as data. These images are stored in `.pickle` files which have been graciously provided by Satya Mallick. Each `.pickle` file contains\n",
    "\n",
    "* `im1`, `im2`, `im3`, `im4`, ... images.\n",
    "* `l1`, `l2`, `l3`, `l4`, ... light source directions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1: Lambertian Photometric Stereo [8 pts]\n",
    "\n",
    "Implement the photometric stereo technique described in the lecture slides and in Forsyth and Ponce 2.2.4 (*Photometric Stereo: Shape from Multiple Shaded Images*). Your program should have two parts:\n",
    "\n",
    "1. Read in the images and corresponding light source directions, and estimate the surface normals and albedo map.\n",
    "\n",
    "1. Reconstruct the depth map from the surface normals. You should first try the naive scanline-based \"shape by integration\" method described in the book and in lecture. (You are required to implement this.) For comparison, you should also integrate using the Horn technique which is already implemented for you in the `horn_integrate` function. Note that for good results you will often want to run the `horn_integrate` function with 10000-100000 iterations, which will take a while. For your final submission, we will require that you run Horn integration for 1000 (one thousand) iterations or more in each case. But for debugging, it is suggested that you keep the number of iterations low.\n",
    "\n",
    "You will find all the data for this part in `synthetic_data.pickle`. Try using only `im1`, `im2` and `im4` first. Display your outputs as mentioned below.\n",
    "\n",
    "Then use all four images (most accurate).\n",
    "\n",
    "For **each** of the **two above cases** you must output:\n",
    "\n",
    "1. The estimated albedo map.\n",
    "\n",
    "1. The estimated surface normals by showing both\n",
    "    1. Needle map, and\n",
    "    1. Three images showing each of the surface normal components.\n",
    "\n",
    "1. A wireframe of the depth map given by the scanline method.\n",
    "\n",
    "1. A wireframe of the depth map given by Horn integration.\n",
    "\n",
    "In total, we expect 2 * 7 = 14 images for this part.\n",
    "\n",
    "An example of outputs is shown in the figure below. (The example outputs only include one depth map, although we expect two â€“ see above.)\n",
    "\n",
    "![Problem 1.1 example outputs](problem1_example.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Setup\n",
    "import pickle\n",
    "import numpy as np\n",
    "from time import time\n",
    "from skimage import io\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "### Example: how to read and access data from a .pickle file\n",
    "pickle_in = open(\"synthetic_data.pickle\", \"rb\")\n",
    "data = pickle.load(pickle_in, encoding=\"latin1\")\n",
    "\n",
    "# data is a dict which stores each element as a key-value pair. \n",
    "print(\"Keys: \", list(data.keys()))\n",
    "\n",
    "# To access the value of an entity, refer to it by its key.\n",
    "for i in range(1, 5):\n",
    "    print(\"\\nImage %d:\" % i)\n",
    "    plt.imshow(data[\"im%d\" % i], cmap=\"gray\")\n",
    "    plt.show()\n",
    "    print(\"Light source direction: \" + str(data[\"l%d\" % i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the above images, can you interpret the orientation of the coordinate frame? If we label the axes in order as x, y, z, then the x-axis points left, the y-axis points up, and the z-axis points out of the screen in our direction. (That means this is a left-handed coordinate system. How will this affect the scanline integration algorithm? Hint: if you integrate rightward along the x-axis and downward along the y-axis, you will be doing in opposite directions to the axes, and the partial derivatives you compute may need to be modified.)\n",
    "\n",
    "_Note: as clarification, no direct response is needed for this cell._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.signal import convolve\n",
    "\n",
    "def horn_integrate(gx, gy, mask, niter):\n",
    "    \"\"\"\n",
    "    horn_integrate recovers the function g from its partial \n",
    "    derivatives gx and gy. \n",
    "    mask is a binary image which tells which pixels are \n",
    "    involved in integration. \n",
    "    niter is the number of iterations. \n",
    "    typically 100,000 or 200,000, \n",
    "    although the trend can be seen even after 1000 iterations.\n",
    "    \"\"\"\n",
    "    g = np.ones(np.shape(gx))\n",
    "    \n",
    "    gx = np.multiply(gx, mask)\n",
    "    gy = np.multiply(gy, mask)\n",
    "    \n",
    "    A = np.array([[0,1,0],[0,0,0],[0,0,0]]) #y-1\n",
    "    B = np.array([[0,0,0],[1,0,0],[0,0,0]]) #x-1\n",
    "    C = np.array([[0,0,0],[0,0,1],[0,0,0]]) #x+1\n",
    "    D = np.array([[0,0,0],[0,0,0],[0,1,0]]) #y+1\n",
    "    \n",
    "    d_mask = A + B + C + D\n",
    "    \n",
    "    den = np.multiply(convolve(mask,d_mask,mode=\"same\"),mask)\n",
    "    den[den == 0] = 1\n",
    "    rden = 1.0 / den\n",
    "    mask2 = np.multiply(rden, mask)\n",
    "    \n",
    "    m_a = convolve(mask, A, mode=\"same\")\n",
    "    m_b = convolve(mask, B, mode=\"same\")\n",
    "    m_c = convolve(mask, C, mode=\"same\")\n",
    "    m_d = convolve(mask, D, mode=\"same\")\n",
    "    \n",
    "    term_right = np.multiply(m_c, gx) + np.multiply(m_d, gy)\n",
    "    t_a = -1.0 * convolve(gx, B, mode=\"same\")\n",
    "    t_b = -1.0 * convolve(gy, A, mode=\"same\")\n",
    "    term_right = term_right + t_a + t_b\n",
    "    term_right = np.multiply(mask2, term_right)\n",
    "    \n",
    "    for k in range(niter):\n",
    "        g = np.multiply(mask2, convolve(g, d_mask, mode=\"same\")) + term_right\n",
    "    \n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def photometric_stereo(images, lights, mask, horn_niter=25000):\n",
    "    \n",
    "    \"\"\"mask is an optional parameter which you are encouraged to use.\n",
    "    It can be used e.g. to ignore the background when integrating the normals.\n",
    "    It should be created by converting the images to grayscale, averaging them,\n",
    "    normalizing to [0, 1] and thresholding (only using locations for which the\n",
    "    pixel value is above some threshold).\n",
    "    \n",
    "    The choice of threshold is something you can experiment with,\n",
    "    but in practice something like 0.05 or 0.1 tends to work well.\n",
    "    \n",
    "    You do not need to use the mask for 1a (it shouldn't matter),\n",
    "    but you SHOULD use it to filter out the background for the specular data (1c).\n",
    "    \"\"\"\n",
    "\n",
    "    \"\"\" ==========\n",
    "    YOUR CODE HERE\n",
    "    ========== \"\"\"\n",
    "\n",
    "    # note:\n",
    "    # images : (n_ims, h, w)\n",
    "    # lights : (n_ims, 3)\n",
    "    # mask   : (h, w)\n",
    "    \n",
    "    albedo = np.ones(images[0].shape)\n",
    "    normals = np.dstack((np.zeros(images[0].shape),\n",
    "                         np.zeros(images[0].shape),\n",
    "                         np.ones(images[0].shape)))\n",
    "    H = np.ones(images[0].shape)\n",
    "    H_horn = np.ones(images[0].shape)\n",
    "    return albedo, normals, H, H_horn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "pickle_in = open(\"synthetic_data.pickle\", \"rb\")\n",
    "data = pickle.load(pickle_in, encoding=\"latin1\")\n",
    "\n",
    "lights = np.vstack((data[\"l1\"], data[\"l2\"], data[\"l4\"]))\n",
    "# lights = np.vstack((data[\"l1\"], data[\"l2\"], data[\"l3\"], data[\"l4\"]))\n",
    "\n",
    "images = []\n",
    "images.append(data[\"im1\"])\n",
    "images.append(data[\"im2\"])\n",
    "# images.append(data[\"im3\"])\n",
    "images.append(data[\"im4\"])\n",
    "images = np.array(images)\n",
    "\n",
    "mask = np.ones(data[\"im1\"].shape)\n",
    "\n",
    "albedo, normals, depth, horn = photometric_stereo(images, lights, mask)\n",
    "\n",
    "# --------------------------------------------------------------------------\n",
    "# The following code is just a working example so you don't get stuck with any\n",
    "# of the graphs required. You may want to write your own code to align the\n",
    "# results in a better layout. You are also free to change the function\n",
    "# however you wish; just make sure you get all of the required outputs.\n",
    "# --------------------------------------------------------------------------\n",
    "\n",
    "def visualize(albedo, normals, depth, horn):\n",
    "    # Stride in the plot, you may want to adjust it to different images\n",
    "    stride = 15\n",
    "\n",
    "    # showing albedo map\n",
    "    fig = plt.figure()\n",
    "    albedo_max = albedo.max()\n",
    "    albedo = albedo / albedo_max\n",
    "    plt.imshow(albedo, cmap=\"gray\")\n",
    "    plt.show()\n",
    "\n",
    "    # showing normals as three separate channels\n",
    "    figure = plt.figure()\n",
    "    ax1 = figure.add_subplot(131)\n",
    "    ax1.imshow(normals[..., 0])\n",
    "    ax2 = figure.add_subplot(132)\n",
    "    ax2.imshow(normals[..., 1])\n",
    "    ax3 = figure.add_subplot(133)\n",
    "    ax3.imshow(normals[..., 2])\n",
    "    plt.show()\n",
    "\n",
    "    # showing normals as quiver\n",
    "    X, Y, _ = np.meshgrid(np.arange(0,np.shape(normals)[0], 15),\n",
    "                          np.arange(0,np.shape(normals)[1], 15),\n",
    "                          np.arange(1))\n",
    "    X = X[..., 0]\n",
    "    Y = Y[..., 0]\n",
    "    Z = depth[::stride,::stride].T\n",
    "    NX = normals[..., 0][::stride,::-stride].T\n",
    "    NY = normals[..., 1][::-stride,::stride].T\n",
    "    NZ = normals[..., 2][::stride,::stride].T\n",
    "    fig = plt.figure(figsize=(5, 5))\n",
    "    ax = fig.gca(projection='3d')\n",
    "    plt.quiver(X,Y,Z,NX,NY,NZ, length=10)\n",
    "    plt.show()\n",
    "\n",
    "    # plotting wireframe depth map\n",
    "    H = depth[::stride,::stride]\n",
    "    fig = plt.figure()\n",
    "    ax = fig.gca(projection='3d')\n",
    "    ax.plot_surface(X,Y, H.T)\n",
    "    plt.show()\n",
    "\n",
    "    H = horn[::stride,::stride]\n",
    "    fig = plt.figure()\n",
    "    ax = fig.gca(projection='3d')\n",
    "    ax.plot_surface(X,Y, H.T)\n",
    "    plt.show()\n",
    "\n",
    "visualize(albedo, normals, depth, horn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Don't forget to run your photometric stereo code on TWO sets of images!\n",
    "# (One being {im1, im2, im4}, and the other being {im1, im2, im3, im4}.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: Specularity Removal [6 pts]\n",
    "\n",
    "Implement the specularity removal technique described in *Beyond Lambert: Reconstructing Specular Surfaces Using Color* (by Mallick, Zickler, Kriegman, and Belhumeur; CVPR 2005).\n",
    "\n",
    "Your program should input an RGB image and light source color and output the corresponding SUV image.  \n",
    "\n",
    "Try this out first with the specular sphere images and then with the pear images.  \n",
    "  \n",
    "For each of the specular sphere and pear images, include\n",
    "\n",
    "1. The original image (in RGB colorspace).\n",
    "\n",
    "1. The recovered $S$ channel of the image.\n",
    "\n",
    "1. The recovered diffuse part of the image. Use $G = \\sqrt{U^2+V^2}$ to represent the diffuse part.\n",
    "\n",
    "In total, we expect 2 * 3 = 6 images as outputs for this problem.\n",
    "\n",
    "Note: You will find all the data for this part in `specular_sphere.pickle` and `specular_pear.pickle`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_rot_mat(rot_v, unit=None):\n",
    "    '''\n",
    "    Takes a vector and returns the rotation matrix required to align the\n",
    "    unit vector(2nd arg) to it.\n",
    "    '''\n",
    "    if unit is None:\n",
    "        unit = [1.0, 0.0, 0.0]\n",
    "    \n",
    "    rot_v = rot_v/np.linalg.norm(rot_v)\n",
    "    uvw = np.cross(rot_v, unit) # axis of rotation\n",
    "\n",
    "    rcos = np.dot(rot_v, unit) # cos by dot product\n",
    "    rsin = np.linalg.norm(uvw) # sin by magnitude of cross product\n",
    "\n",
    "    # normalize and unpack axis\n",
    "    if not np.isclose(rsin, 0):\n",
    "        uvw = uvw/rsin\n",
    "    u, v, w = uvw\n",
    "\n",
    "    # compute rotation matrix \n",
    "    R = (\n",
    "        rcos * np.eye(3) +\n",
    "        rsin * np.array([\n",
    "            [ 0, -w,  v],\n",
    "            [ w,  0, -u],\n",
    "            [-v,  u,  0]\n",
    "        ]) +\n",
    "        (1.0 - rcos) * uvw[:,None] * uvw[None,:]\n",
    "    )\n",
    "    return R\n",
    "\n",
    "def RGBToSUV(I_rgb, rot_vec):\n",
    "    '''\n",
    "    Your implementation which takes an RGB image and a vector encoding\n",
    "    the orientation of the S channel w.r.t. to RGB.\n",
    "    '''\n",
    "\n",
    "    \"\"\" ==========\n",
    "    YOUR CODE HERE\n",
    "    ========== \"\"\"\n",
    "\n",
    "    S = np.ones(I_rgb.shape[:2])\n",
    "    G = np.ones(I_rgb.shape[:2])\n",
    "    return S, G\n",
    "\n",
    "pickle_in = open(\"specular_sphere.pickle\", \"rb\")\n",
    "data = pickle.load(pickle_in, encoding=\"latin1\")\n",
    "\n",
    "# sample input\n",
    "S, G = RGBToSUV(data[\"im1\"], np.hstack((data[\"c\"][0][0],\n",
    "                                        data[\"c\"][1][0],\n",
    "                                        data[\"c\"][2][0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3: Robust Photometric Stereo [6 pts]\n",
    "\n",
    "Now we will perform photometric stereo on our sphere/pear images which include specularities. First, for comparison, run your photometric stereo code from 1a on the original images (converted to grayscale and rescaled/shifted to be in the range [0, 1]). You should notice erroneous \"bumps\" in the resulting reconstructions, as a result of violating the Lambertian assumption. For this, show the same outputs as in 1a.\n",
    "\n",
    "Next, combine parts 1 and 2 by removing the specularities (using your code from 1b) and then running photometric stereo on the diffuse components of the specular sphere/pear images. Our goal will be to remove the bumps/sharp parts in the reconstruction.\n",
    "\n",
    "For the specular sphere image set in `specular_sphere.pickle`, using all of the four images (again, be sure to convert them to grayscale and normalize them so that their values go from 0 to 1), include:\n",
    "\n",
    "1. The estimated albedo map (original and diffuse).\n",
    "\n",
    "1. The estimated surface normals (original and diffuse) by showing both\n",
    "\n",
    "    1. Needle map, and\n",
    "    1. Three images showing each of the surface normal components.\n",
    "    \n",
    "1. A wireframe of depth map (original and diffuse).\n",
    "\n",
    "1. A wireframe of the depth map given by Horn integration (original and diffuse).\n",
    "\n",
    "In total, we expect 2 \\* 7 = 14 images for the 1a comparison, plus 2 \\* 7 = 14 images for the outputs after specularity removal has been performed. (Thus 28 output images overall.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------------\n",
    "# You may reuse the code for photometric_stereo here.\n",
    "# Write your code below to process the data and send it to photometric_stereo\n",
    "# and display the albedo, normals, and depth maps.\n",
    "# ---------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2: Image Filtering [13 pts]\n",
    "\n",
    "### Part 1: Warmup [1.5 pts]\n",
    "\n",
    "In this problem, we expect you to use convolution to filter the provided image with three different types of kernels:\n",
    "\n",
    "1. A 5x5 Gaussian filter with $\\sigma = 5$.\n",
    "2. A 31x31 Gaussian filter with $\\sigma = 5$.\n",
    "3. A sharpening filter.\n",
    "\n",
    "This is the image you will be using:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Open image as grayscale\n",
    "dog_img = io.imread('dog.jpg', as_gray=True)\n",
    "\n",
    "# Show image\n",
    "plt.imshow(dog_img, cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For convenience, we have provided a helper function for creating a square isotropic Gaussian kernel. We have also provided the sharpening kernel that you should use. Finally, we have provided a function to help you plot the original and filtered results side-by-side. Take a look at each of these before you move on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def gaussian2d(filter_size=5, sig=1.0):\n",
    "    \"\"\"Creates a 2D Gaussian kernel with\n",
    "    side length `filter_size` and a sigma of `sig`.\"\"\"\n",
    "    ax = np.arange(-filter_size // 2 + 1., filter_size // 2 + 1.)\n",
    "    xx, yy = np.meshgrid(ax, ax)\n",
    "    kernel = np.exp(-0.5 * (np.square(xx) + np.square(yy)) / np.square(sig))\n",
    "    return kernel / np.sum(kernel)\n",
    "\n",
    "sharpening_kernel = np.array([\n",
    "    [1, 4,     6,  4, 1],\n",
    "    [4, 16,   24, 16, 4],\n",
    "    [6, 24, -476, 24, 6],\n",
    "    [4, 16,   24, 16, 4],\n",
    "    [1,  4,    6,  4, 1],\n",
    "]) * -1.0 / 256.0\n",
    "\n",
    "def plot_results(original, filtered):\n",
    "    # Plot original image\n",
    "    plt.subplot(2,2,1)\n",
    "    plt.imshow(original, vmin=0.0, vmax=1.0)\n",
    "    plt.title('Original')\n",
    "    plt.axis('off')\n",
    "\n",
    "    # Plot filtered image\n",
    "    plt.subplot(2,2,2)\n",
    "    plt.imshow(filtered, vmin=0.0, vmax=1.0)\n",
    "    plt.title('Filtered')\n",
    "    plt.axis('off')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now fill in the functions below and display outputs for each of the filtering results. There should be three sets of (original, filtered) outputs in total. You are allowed to use the imported `convolve` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from scipy.signal import convolve\n",
    "\n",
    "def filter1(img):\n",
    "    \"\"\"Convolve the image with a 5x5 Gaussian filter with sigma=5.\"\"\"\n",
    "    \"\"\" ==========\n",
    "    YOUR CODE HERE\n",
    "    ========== \"\"\"\n",
    "    return np.ones_like(img)\n",
    "\n",
    "def filter2(img):\n",
    "    \"\"\"Convolve the image with a 31x31 Gaussian filter with sigma=5.\"\"\"\n",
    "    \"\"\" ==========\n",
    "    YOUR CODE HERE\n",
    "    ========== \"\"\"\n",
    "    return np.ones_like(img)\n",
    "\n",
    "def filter3(img):\n",
    "    \"\"\"Convolve the image with the provided sharpening filter.\"\"\"\n",
    "    \"\"\" ==========\n",
    "    YOUR CODE HERE\n",
    "    ========== \"\"\"\n",
    "    return np.ones_like(img)\n",
    "\n",
    "for filter_name, filter_fn in [\n",
    "    ('5x5 Gaussian filter, sigma=5', filter1),\n",
    "    ('31x31 Gaussian filter, sigma=5', filter2),\n",
    "    ('sharpening filter', filter3),\n",
    "]:\n",
    "    filtered = filter_fn(dog_img)\n",
    "    print(filter_name)\n",
    "    plot_results(dog_img, filtered)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2 [2 pts]\n",
    "\n",
    "Display the Fourier log-magnitude transform image for the (original image, 31x31 Gaussian-filtered image) pair. (No need to include the others.) We have provided the code to compute the Fourier log-magnitude image.\n",
    "\n",
    "Then, as a text answer, explain the differences you see between the original frequency domain image and the 31x31 Gaussian-filtered frequency domain image. In particular, be sure to address the following points:\n",
    "- Why is most of the frequency visualization dark after applying the Gaussian filter, and what does this signify?\n",
    "- What is an example of one of these dark images in the spatial domain (original image)?\n",
    "- What do the remaining bright regions in the magnitude image represent?\n",
    "- What is an example of one of these bright regions in the spatial domain (original image)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Visualize the frequency domain images\n",
    "\n",
    "def plot_ft_results(img1, img2):\n",
    "    plt.subplot(2,2,1)\n",
    "    plt.imshow(img1, cmap='gray')\n",
    "    plt.title('Original FT log-magnitude')\n",
    "    plt.axis('off')\n",
    "\n",
    "    plt.subplot(2,2,2)\n",
    "    plt.imshow(img2, cmap='gray')\n",
    "    plt.title('Filtered FT log-magnitude')\n",
    "    plt.axis('off')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "def ft_log_magnitude(img_gray):\n",
    "    return np.log(np.abs(np.fft.fftshift(np.fft.fft2(img_gray))))\n",
    "\n",
    "print('31x31 Gaussian filter, sigma=5')\n",
    "plot_ft_results(ft_log_magnitude(dog_img), ft_log_magnitude(filter2(dog_img)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your answer to Problem 2.2:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3 [3 pts]\n",
    "\n",
    "Consider (1) smoothing an image with a 3x3 box filter and then computing the derivative in the y-direction (use the derivative filter from Lecture 7). Also consider (2) computing the derivative first, then smoothing. What is a single convolution kernel that will simultaneously implement both (1) and (2)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your answer to Problem 2.3:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 4 [3 pts]\n",
    "\n",
    "Give an example of a 3x3 separable filter and compare the number of arithmetic operations it takes to\n",
    "convolve using that filter on an n Ã— n image before and after separation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your answer to Problem 2.4:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 5: Filters as Templates [3.5 pts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose that you are a clerk at a grocery store. One of your responsibilites is to check the shelves periodically and stock them up whenever there are sold-out items. You got tired of this laborious task and decided to build a computer vision system that keeps track of the items on the shelf.\n",
    "\n",
    "Luckily, you have learned in CSE 252A (or are learning right now) that convolution can be used for template matching: a template g is multiplied with regions of a larger image f to measure how similar each region is to the template. Note that you will want to flip the filter before giving it to your convolution function, so that it is overall not flipped when making comparisons. You will also want to subtract off the mean value of the image or template (whichever you choose, subtract the same value from both the image and template) so that your solution is not biased toward higher-intensity (white) regions.\n",
    "\n",
    "The template of a product (template.jpg) and the image of the shelf (shelf.jpg) is provided. We will use convolution to find the product in the shelf.\n",
    "\n",
    "<img src=\"template.jpg\" alt=\"template\" width=\"25px\"/>\n",
    "<img src=\"shelf.jpg\" alt=\"shelf\" width=\"600px\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from skimage import io\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.signal import convolve\n",
    "%matplotlib inline\n",
    "\n",
    "# Load template and image in grayscale\n",
    "img = io.imread('shelf.jpg')\n",
    "img_gray = io.imread('shelf.jpg', as_gray=True)\n",
    "temp = io.imread('template.jpg')\n",
    "temp_gray = io.imread('template.jpg', as_gray=True)\n",
    "\n",
    "# Perform a convolution between the image and the template\n",
    "\"\"\" ==========\n",
    "YOUR CODE HERE\n",
    "========== \"\"\"\n",
    "out = np.zeros_like(img_gray)\n",
    "\n",
    "# Find the location with maximum similarity\n",
    "y, x = (np.unravel_index(out.argmax(), out.shape))\n",
    "\n",
    "# Display product template\n",
    "plt.figure(figsize=(20,16))\n",
    "plt.subplot(3, 1, 1)\n",
    "plt.imshow(temp)\n",
    "plt.title('Template')\n",
    "plt.axis('off')\n",
    "\n",
    "# Display convolution output\n",
    "plt.subplot(3, 1, 2)\n",
    "plt.imshow(out)\n",
    "plt.title('Convolution output (white means more correlated)')\n",
    "plt.axis('off')\n",
    "\n",
    "# Display image\n",
    "plt.subplot(3, 1, 3)\n",
    "plt.imshow(img)\n",
    "plt.title('Result (blue marker on the detected location)')\n",
    "plt.axis('off')\n",
    "\n",
    "# Draw marker at detected location\n",
    "plt.plot(x, y, 'bx', ms=40, mew=10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3: Edge Detection [7 pts]\n",
    "\n",
    "In this problem, you will write a function to perform edge detection. The following steps need to be implemented.\n",
    "\n",
    "- **Smoothing [1 pt]:** First, we need to smooth the images to prevent noise from being considered edges. For this problem, use a 9x9 Gaussian kernel filter with $\\sigma = 1.4$ to smooth the images.\n",
    "\n",
    "- **Gradient Computation [2 pts]:** After you have finished smoothing, find the image gradient in the horizontal and vertical directions. Compute the gradient magnitude image as $|G| = \\sqrt{G_x^2 + G_y^2}$.\n",
    "\n",
    "- **Non-Maximum Suppression [4 pts]:** We would like our edges to be sharp, unlike the ones in the gradient magnitude image from above. Use local non-maximum suppression on the gradient magnitude image to suppress all local non-maximum gradient magnitude values. To see how it affects the results, try using two different window sizes: 5x5 and 21x21.\n",
    "\n",
    "Compute the images after each step. Show each of the intermediate steps and label your images accordingly.\n",
    "\n",
    "In total, there should be five output images (original, smoothed, gradient magnitude, NMS result with 5x5 window, NMS result with 21x21 window).\n",
    "\n",
    "**For this question, use the image `geisel.jpeg`.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from skimage import io\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "def smooth(image):\n",
    "    \"\"\" ==========\n",
    "    YOUR CODE HERE\n",
    "    ========== \"\"\"\n",
    "    return image\n",
    "\n",
    "def gradient(image):\n",
    "    \"\"\" ==========\n",
    "    YOUR CODE HERE\n",
    "    ========== \"\"\"\n",
    "    g_mag = np.zeros_like(image)\n",
    "    g_theta = np.zeros_like(image)\n",
    "    return g_mag, g_theta\n",
    "\n",
    "def nms(g_mag, g_theta, window_size=5):\n",
    "    \"\"\" ==========\n",
    "    YOUR CODE HERE\n",
    "    ========== \"\"\"\n",
    "    return image\n",
    "\n",
    "def edge_detect(image):\n",
    "    \"\"\"Perform edge detection on the image.\"\"\"\n",
    "    smoothed = smooth(image)\n",
    "    g_mag, g_theta = gradient(smoothed)\n",
    "    nms_image_5x5 = nms(g_mag, g_theta, window_size=5)\n",
    "    nms_image_21x21 = nms(g_mag, g_theta, window_size=21)\n",
    "    return smoothed, g_mag, nms_image_5x5, nms_image_21x21\n",
    "\n",
    "# Load image in grayscale\n",
    "image = io.imread('geisel.jpeg', as_gray=True)\n",
    "smoothed, g_mag, nms_image_5x5, nms_image_21x21 = edge_detect(image)\n",
    "\n",
    "print('Original:')\n",
    "plt.imshow(image)\n",
    "plt.show()\n",
    "\n",
    "print('Smoothed:')\n",
    "plt.imshow(smoothed)\n",
    "plt.show()\n",
    "\n",
    "print('Gradient magnitude:')\n",
    "plt.imshow(g_mag)\n",
    "plt.show()\n",
    "\n",
    "print('NMS with 5x5 window:')\n",
    "plt.imshow(nms_image_5x5)\n",
    "plt.show()\n",
    "\n",
    "print('NMS with 21x21 window:')\n",
    "plt.imshow(nms_image_21x21)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Submission Instructions\n",
    "Remember to submit a PDF version of this notebook to Gradescope. Please make sure the contents of each cell are clearly shown in your final PDF file. **If they are not, we may dock points.**\n",
    "\n",
    "There are multiple options for converting the notebook to PDF:\n",
    "1. You can export using LaTeX (File $\\rightarrow$ Download as $\\rightarrow$ PDF via LaTeX).\n",
    "2. You can first export as HTML and then save it as a PDF."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py36",
   "language": "python",
   "name": "py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
